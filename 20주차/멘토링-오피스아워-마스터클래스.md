# 멘토링

프로젝트 아이디어 잡기
- 평소에 관심있는 주제로 한다.

프로젝트 상세를 작성하는 법
- 베이스라인, 메트릭 선정 이유 작성
- 문제 -> 어려움 -> 해결 -> 회고

# NLP 오피스 아워
NLP 모델링 순서
- RNN >> Seq2Seq -> Transformer 순으로 모델링

Transformer 관련
- Transformer 인코더-디코더 개수 : 3-1, 6-3, 12-6
- Positional embedding 제거하면 성능 향상
- (멘토님 추측) LM이 안하녕하세요 만들어내면 인간이 이해는 할 수 있어

데이터 나누기
- Tran/Val/Test 근거를 바탕으로 잘 나누기

모델 초매개변수
- Layer size 설정
- Dropout : 0.7부터 시작
    - 데이터가 적기 때문에
    - 0.7 >> 0.5 >> 0.3
- Weight decay : 1e-4 ~ 1e-5
    - 주로 1e-5 사용
- Teacher forcing : Teacher forcing 비율을 scheduling하면서 줄임
    - Transformer 학습 시 90%~100% >> 낮춰감
    - 디코딩할 때마다 teacher forcing을 결정할 수 있음 
- init LR
    - warmpup 최대 점 : ~ 1e-3
    - batch size가 클 때는 LR이 높아야 일반화가 잘되는 학습이 됨
        - 튕김 방지를 위해 Warmp 스케쥴링 사용
        - 논문 : 
- Batch bucketing : 비슷한 길이 데이터를 묶음
    - 학습 속도 향상
    - 평균 패딩이 줄어듦
- Transformer 학습하다 Loss가 NaN일 때
    - 이유 : forward pass의 계산 정밀도가 낮아서
    - 계산 정밀도 : FP16 < FP32 
    - Mixed Precision
        - 텐서코어가 있을 때만 사용 가능(Volta, Amphere)
        - forward : FP16
        - backward : FP32
    - 해결 방법
        1. LR을 줄이면 됨
        2. Warmup을 길게 함
        3. LR을 낮추면서 Batch를 줄임
        4. 꼼수 : 시드를 바꿈, 확률 10%
- warmup paramerter 저장했다가 학습에 사용해도 좋음

디버깅 팁
1. 코드 구조 명확히 알 것
2. 데이터 단순히
    - 10개 데이터를 1개의 batch로 사용
3. CPU로 돌리면 오류 디버깅이 용이

finetuning할 때 의 layer freeze에 대한 tip
- freeze하면 학습 시간 줄어듦
- 일반적으로 feature 추출하는 layer를 freeze하는 편
- layer freeze 하는 비율을 bayes search로 찾는 건 어떻게 생각
    - 좋으면 사용

프로젝트 Tip
- 플랜 B 만들기
- 주석없이도 설명가능한 코드가 Best
    - 변수명, 함수명 잘 짓기
    - 타입 힌트 많이 쓰기


# 오피스 아워

기업의 상용 서비스는 추론 서버에도 GPU를 사용하는 경우가 많나요? 아니면 CPU 추론을 하는 경우가 많나요?
- API call에 드는 비용(시간, 금전)에 따라 다르다
- 예: CPU 0.5초 > GPU 0.3초

테스트의 종류
- 로드, 퍼포먼스 테스트 : 코드의 성능, 수행시간에 대한 테스트
- e2e 테스트 : end-to end 테스트, 가장 중요한 테스트, 최소한의 테스트 코드

