# 멘토링

프로젝트 아이디어 잡기
- 평소에 관심있는 주제로 한다.

프로젝트 상세를 작성하는 법
- 베이스라인, 메트릭 선정 이유 작성
- 문제 -> 어려움 -> 해결 -> 회고

# NLP 오피스 아워
## 모델링 Tip
LR 스케쥴러
- Transformer에서 자주 사용하는 스케쥴러
    - Noam : 쓰기 쉬움, 처음 많이 사용
    - Cosine annealing : 성능 좋음 그러나 쓰기 어려움, warmup에 따라서 학습 시간 달라짐
        - warmup 길면 학습 시간도 길어짐
        - epoch : 10epoch이면 길다
            - 트랜스포머 파라미터 미세하게 조정되서 lr을 낮게 시작해서 warmup도 길게 설정
            - RNN, Seq2Seq 는 1e-5되면 학습 잘 안 되는 경향
            - 트랜스포머는 경우에 따라서 1e-8까지도 학습이 가능
- RNN
    - Warmup 없이 step한 스케쥴러(stepLR) 사용

데이터 적을 때
- 트랜스포머보다 RNN이 좋음
- RNN이 학습이 쉬움

트랜스포머 학습 잘 안될 때
1. 모델의 레이어 차원이나 깊이를 줄인다
    - 64(Dim) x 1(디코더 개수) x 3(Depth)), 
    - 보통 논문 128 (Dim), 6 (Encoder개수), 3(Decoder 개수) => 꼭 그럴 필요 없다
2. RNN으로 바꾸기
    - 빠른 문제 해결이 가능함

NLP 모델링 순서
- RNN -> Seq2Seq -> Transformer 순으로 모델링

데이터 나누기
- Tran/Val/Test 근거를 바탕으로 잘 나누기

모델 초매개변수
Layer
- Layer depth & Layer size
    1. Layer size 먼저 정하고 => 2의 배수 or 8의 배수
        - 논문의 데이터 사이즈와 사용한 파라미터를 우리 문제와 비교해서 각을 잡자
        - 여러 벤치마크를 볼 것
        - 벤치마크 데이터 고르는 것은 유사한 것을 찾자
    2. Layer size 256 -> 128(RNN 적용할까?) -> 64 -> 32
    3. train/val(학습 비교)/test(모델의 진짜 성능 비교) 반드시 구분할 것
    4. Layer depth 
        - Transformer 인코더-디코더 개수 : 3-1, 6-3, 12-6
        - Language Model(LM) 만들 때 Positional embedding 제거하면 성능 향상
        - (멘토님 추측) LM이 안하녕세요 만들어내도 인간이 이해는 할 수 있어
학습이 되면 Dropout, Weight decay 적용
- Dropout : 0.7부터 시작
    - 데이터가 적기 때문에
    - (멘토님 옵션) 0.7 >> 0.5 >> 0.3
- Weight decay : 1e-4 ~ 1e-5
    - 주로 1e-5 사용
    - 1e-3는 적은 편
- [Teacher forcing](https://blog.naver.com/PostView.naver?blogId=sooftware&logNo=221790750668) : Teacher forcing 비율을 scheduling하면서 줄임, 모델 성능이 향상됨
    - 캡처 이미지 참고
    - 디코딩할 때마다 teacher forcing을 결정할 수 있음
    - Transformer 학습 시 90%~100% >> 낮춰감
- init LR
    - warmpup 최대 점 : ~ 1e-3
        - batch size가 클 때는 LR이 높아야 일반화가 잘되는 학습이 됨
            - 발산 방지를 위해 Warmp 스케쥴링 사용
                - 배치 크기 크다고 무조건 성능이 좋은 건 아님
                - 배치 크기가 작으면 낮은 LR로도 학습 가능
                - 배치 크게 가져갈 거면 warup 필수 사용
- Batch bucketing : 비슷한 길이 데이터를 묶음
    - 캡처2 이미지 참고 
    - 평균 10~15% 학습 속도 향상
    - 평균 패딩이 줄어듦
    - 음성인식 bucketing이 잘 안돼
        - 길이가 긴 음성은 같은 패턴이 반복되기 때문
        - 음성마다 다른 패턴이 보이면 사용해도 OK
- Transformer 학습하다 Loss가 NaN일 때
    - 이유 : Mixed Precision 때문에 `forward pass의 계산 정밀도가 낮아서`
    - 계산 정밀도 : FP16 < FP32 
    - Mixed Precision : 현업 필수
        - 쿠다코어는 FP32로만 계산하게 되어있음
        - 텐서코어가 있는 GPU만 AMP 사용 가능(GPU 구조가 Volta, Amphere)
            - FP16의 매트릭스 2개 곱하고 FP16 혹은 FP32를 더해서 행렬의 곱,합 연산
            - forward : FP16, 속도가 빨라짐
            - backward : FP32
    - 해결 방법
        1. Init LR을 줄임
        2. Warmup을 길게 함
        3. LR을 낮추면서 Batch를 줄임
        4. 꼼수 : 시드를 바꿈, 확률 10%
- 사파적 방법 : warmup 건 paramerter 저장했다가 학습할 때마다 불러와서 사용해도 좋음

디버깅 팁
1. 코드 구조 명확하게 만들 것
    - 데이터셋 -> 데이터로더 -> 모델
2. 데이터를 단순하게 만들 것
    - 10개 데이터를 1개의 batch로 사용
3. CPU로 돌리면 오류 디버깅이 용이
    - GPU/Mutli GPU로 돌리면 오류 메시지가 제대로 반환되지 않는 경우 존재

finetuning할 때 의 layer freeze에 대한 tip
- freeze하면 학습 시간 줄어듦
- 일반적으로 feature 추출하는 layer를 freeze하는 편
- layer freeze 하는 비율을 bayes search로 찾는 건 어떻게 생각
    - 좋으면 사용하지만 오래 걸릴 듯

프로젝트 Tip
- 플랜 B 만들기
- 주석없이도 설명가능한 코드가 Best
    - 변수명, 함수명 잘 짓기
    - 타입 힌트 많이 쓰기


# 오피스 아워

기업의 상용 서비스는 추론 서버에도 GPU를 사용하는 경우가 많나요? 아니면 CPU 추론을 하는 경우가 많나요?
- API call에 드는 비용(시간, 금전)에 따라 다르다
- 예: CPU 0.5초, GPU 0.3초면 CPU를 사용.

테스트의 종류
- 로드, 퍼포먼스 테스트 : 코드의 성능, 수행시간에 대한 테스트
- e2e 테스트 : end-to end 테스트, 가장 중요한 테스트, 최소한의 테스트 코드

# Genetic algorithm

GA 흐름
1. Initialization
2. Fitness assignment : 결과값 scalar, 속도 높이는 데 중요
3. Selection : 자손을 만든다.
4. Crossowver : NeroEvoltuion에서는 대부분 생략
5. Mutation 
6. Stoping criteria -> Y/N

최근 강화학습으로 최적화 문제를 해결하기 위한 연구가 많음
- Conv NeroEvolution
- SANE
- ESP
- NEAT
- ES : 네트워크 가중치를 정규분포를 따르게 하고 crossover 생략하고 mutation만 진행